2019-November-08 - dev-meeting
==============================

### ranges & subranges

#### need

Recently on some data-cookbook work, I had a need to hit the Sierra API many, many times, and, not surprisingly, thought this might be a good use of trio/asynchronous-programming.

(Note, turns out I should be able to make far fewer calls to the Sierra API than I'd thought.)

I took a subset of the 100,000+ item-ids, yielding 3 files, each with 100 item-ids.

I wanted to have a configurable number of workers operating at the same time, and though I've utilized a couple of different advanced trio-techniques for managing such situations ([CapacityLimiters](https://trio.readthedocs.io/en/latest/reference-core.html#trio.CapacityLimiter) and [Semaphores](https://trio.readthedocs.io/en/stable/reference-core.html#trio.Semaphore)) -- I thought of something simpler.


#### the idea

Let's take a list of 100 item-ids -- I was making API calls on these. And let's say I wanted to use 6 workers. I wanted an easy way to get the right groupings -- meaning... a list of subgroups of 6, like `[ (0,1,2,3,4,5), (6,7,8,9,10,11), etc. ]`. And I found something cool...


### the implementation

    >>> lst = [i for i in range(100)]
    >>> lst_length = len(lst)
    >>> worker_count = 6

    >>> ranges = list( zip(*(iter(range(lst_length)),) * worker_count) )  # <https://stackoverflow.com/a/1335618>
    >>>
    >>> import pprint
    >>> pprint.pprint( ranges )
    [(0, 1, 2, 3, 4, 5),
     (6, 7, 8, 9, 10, 11),
     (12, 13, 14, 15, 16, 17), ...

Then, in a [_very_ un-elegant way](https://github.com/birkin/sierra_econ_items_project/blob/8160cf52c22e3d959d032c5d7e696d3ca41ef8d4/controller.py#L334-L341) (I suspect `map` could do this more nicely) , I recreated the range and subranges (the tuples) with the actual data. So that would look like the above, but with actual item-ids instead of those integers.

Then, because nurseries are nicely scoped, I figured I'd loop through that list, and for each tuple-section, create a nursery that, iterating through each _element_ of the tuple, would fire off a synchronous API call.

Admittedly, this has a downside: If 5 of the workers are finished, each is unnecessarily waiting on the 6th to complete -- whereas in the more elegant but complicated way, each worker would immediately get to work on the next job.

But it's _simple_.

Because I'm looping through the list, and then looping through each tuple, [in the code](https://github.com/birkin/sierra_econ_items_project/blob/8160cf52c22e3d959d032c5d7e696d3ca41ef8d4/controller.py#L348-L352), I only have one line creating the nursery, and then only one line creating each `start_soon()` worker.

Further, knowing that I had this convenient nursery scope, I decided to save the data from the six worker api-calls, so that if there were a crash, I wouldn't lose much data. This ties the frequency of the autosaves to the number of workers, which isn't ideal, but I figured I'd try this to see the penalty.


### results

The results were impressive: the async method was about 4 times faster, even though it (unlike the sync method) was performing file-saves along the way.

    (sync, w/logging)
    [07/Nov/2019 16:49:59] DEBUG [controller-add_bib_data()::404] get_item_data time_taken, ```0:00:41.477103```

    (sync, no logging)
    (env3_dc) birkin$ python3 ./controller.py add_bib_data
    [07/Nov/2019 16:51:43] INFO [controller-add_bib_data()::404] get_item_data time_taken, ```0:00:40.639323```

    (async, w/logging)
    (env3_dc) birkin$ python3 ./controller.py add_bib_data_asyc
    [07/Nov/2019 16:43:24] DEBUG [controller-add_bib_data()::365] get_item_data time_taken, ```0:00:09.745021```

    (async, no logging)
    (env3_dc) birkin$ python3 ./controller.py add_bib_data_async
    [07/Nov/2019 16:54:55] INFO [controller-add_bib_data_async()::366] get_item_data time_taken, ```0:00:10.009921```


---

---



2019-November-01 - dev-meeting
==============================

### async refactoring

#### background

A few weeks ago, I created an internal demo url-endpoint showing five hits to `https://httpbin.org/delay/x`, where x is .6, .8, 1, 1.2, and 1.4, showing that whereas normal procedural programming would result in the response taking about 5 seconds, in this case it takes closer to 1.5 seconds -- because of the use of the asychronous python library, [trio](https://github.com/python-trio/trio).

I made this proof-of-concept because one of the url endpoints in the [availability-api](https://library.brown.edu/availability_api/v2/bib_items/b1815113/) has to make, behind-the-scenes, four different http calls to obtain the required data for the response. It's fast enough for it's purpose, but this is the kind of use-case that could definitely benefit from an asynchronous refactor.

Reviewing trio, I re-read a _terrific_ blog-post by the trio author, Nathaniel J. Smith, "[Notes on structured concurrency, or: Go statement considered harmful](https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/)". This, truly, is one of my favorite tech-reads of the last year (at least). It posits that modern-day programming-languages don't have the necessary structures for _conceptually_ making async programming as easy as it could be, and explains his approach to this situation, in trio.

#### last weekend

(This is low-priority, so I noodle when I can.) I decided to flesh out the proof-of-concept a bit, by stubbing functions close to the actual code I'd put into place. In undertaking the [first working refactor](https://github.com/Brown-University-Library/availability_api/blob/b1b55752e60e382c40a68dbb3441e57e5211512c/availability_app/lib/bib_items_async_v2.py#L53-L59) (all urls still go to `httpbin.org`; "working" means the flow works), I realized I couldn't make all four calls simultaneously, because two of the calls were dependent on getting an authentication-token. Thus the two-stage implementation where I make two async calls -- one for the toke and one for independent data.

But then I realized that in this implementation, if the token-response was fast, and the independent-data-response was slow, I was implementing an unnecessary wait. So I refactored. There are still two stages, but they're more sensible.

The [first stage](https://github.com/Brown-University-Library/availability_api/blob/master/availability_app/lib/bib_items_async_v2.py#L30-L34) initiates the same two async requests as before.

But this time, I don't wait on the independent-data-response: Now, once the token is obtained, I immediately make the [other two async requests](https://github.com/Brown-University-Library/availability_api/blob/master/availability_app/lib/bib_items_async_v2.py#L74-L77) dependent on the token.

This flow, too works (again, with the `httpbin.org` calls). But the _cool_ thing about this refactor work is that I was able to do 'normal' high-level refactoring thinking without having to get unnecessarily deep into the weeds of the specifics of handling async code. That's exactly the promise of Smith's approach to implement intuitive language-structures for async programming.


---

---
